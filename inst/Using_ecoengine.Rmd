# Guide to using the ecoengine R package

The Berkeley Ecoengine ([http://ecoengine.berkeley.edu](http://ecoengine.berkeley.edu)) provides an open API to a wealth of museum data contained in the Berkeley natural history museums. This R package provides a programmatic interface to this rich repository of data allowing for the data to be readily analyzed and visualized in a variety of contexts. This vignette provides a brief overview of the package's capabilities. 

The API documentation is available at [http://ecoengine.berkeley.edu/developers/](http://ecoengine.berkeley.edu/developers/). As with most APIs all requests return a call that displays all the data endpoints accessible to users. Ecoengine has something similar.

```{r, about, echo = FALSE}
suppressPackageStartupMessages(library(ecoengine))
suppressPackageStartupMessages(library(pander))
```

```{r, about_ee_dont, eval = FALSE, tidy = TRUE}
library(ecoengine)
ee_about()
```

```{r, about_ee, results = "asis", echo = FALSE}
pandoc.table(ee_about(), justify = "left")
```

## The ecoengine class

Most functions in the ecoengine package will return a `S3` object of class `ecoengine`. The class contains 4 items.  

- A total result count (not necessarily the results in this particular object)
- The call (So a reader can replicate the results)  
- The type (`photos`, `observation`, `checklist`, or `sensor`)  
- The data. Data are most often coerced into a `data.frame`. To access the data simply use `result_object$data`.  


## Notes on downloading large data requests

For the sake of speed, results are paginated at `25` results per page. It's possible to request all pages for any query by specifying page = all in any search. However, this should be used if the request is reasonably sized (`1,000` or fewer records). With larger requests, there is a chance that the query might become interrupted and you could lose all the partially downloaded data. Instead, use the returned observations to split the request. You can always check the number of requests you'll need to retreive data for any query by running `ee_pages(obj)` where `obj` is an object of class `ecoengine`.

```{r, pagination, eval = TRUE}
request <- ee_photos(county = "Santa Clara County")
ee_pages(request)
# Now it's simple to parallelize this request
# You can parallelize across number of cores by passing a vector of pages from 1 through total_pages.
```


### Specimen Observations

The database contains over 2 million records. Many of these have already been georeferenced. There are two ways to obtain observations. One is to query the database directly based on a partial or exact taxonomic match. For example

```{r}
pinus_observations <- ee_observations(scientific_name_exact = "Pinus", page = 1)
pinus_observations
```

For additional fields upon which to query, simply look through the help for `?ee_observations`. In addition to narrowing data by taxonomic group, it's also possible to add a bounding box (add argument `bbox`) or request only data that have been georeferenced (set `georeferenced = TRUE`). 

---

### Photos  

```{r, photo_count}
photos <- ee_photos_get(quiet = TRUE)
photos
```
The database currently holds `r photos$count` photos. Photos can be searched by state province, county, genus, scientific name, authors along with date bounds. For additional options see `?ee_photos_get`.


#### Searching photos by author

```{r, photos_by_author, tidy = TRUE, width.cutoff = 60, background = '#F7F7F7'}
charles_results <- ee_photos(author = "Charles Webber")
charles_results
# Let's examine a couple of rows of the data
charles_results$data[1:2, ]
```
---  

#### Browsing these photos

```{r, browsing_photos, eval = FALSE}
view_photos(charles_results)
```
This will launch your default browser and render a page with thumbnails of all images returned by the search query.

![](browse_photos.png)

---  


### Species checklists

There is a wealth of checklists from all the source locations. To get all available checklists from the engine, run: 
  
```{r, checklists}
all_lists  <- ee_checklists()
head(all_lists[, c("footprint", "subject")])
```  
Currently there are `r nrow(all_lists)` lists available. We can drill deeper into any list to get all the available data. We can also narrow our checklist search to groups of interest (see `unique(all_lists$subject)`). For example, to get the list of Spiders:

```{r, checklist_spiders}
spiders  <- ee_checklists(subject = "Spiders")
spiders
```  

Now we can drill deep into each list. For this tutorial I'll just retrieve data from the the two lists returned above.

```{r, checklist_details}
library(plyr)
spider_details <- ldply(spiders$url, checklist_details)
names(spider_details)
unique(spider_details$scientific_name)
```

Our resulting dataset now contains `r length(unique(spider_details$scientific_name))` unique spider species. 



### Sensors


Some notes on the sensors.  

You'll need a sensor's id to query the data for that particular metric and location. The `ee_list_sensors()` function will give you a condensed list with the location, metric, binning method and most importantly the `sensor_id`. You'll need this id for the data retrieval. 

```{r, eval = FALSE}
head(ee_list_sensors())
```

```{r, echo = FALSE, results = "asis"}
pandoc.table(head(ee_list_sensors()), caption = "List of stations")
```

Let's download solar radiation for the Angelo reserve HQ (sensor_id = `1625`).

```{r}
# First we can grab the list of sensor ids
sensor_ids <- ee_list_sensors()$record
# In this case we just need data for sensor with id 1625
angelo_hq <- sensor_ids[1]
results <- ee_sensor_data_get(angelo_hq, page = 2)
results
```  

Notice that the query returned `r format(results$results, nsmall = 0)` observations but has only retrieved the `25-50` since we requested records for page 2 (and each page by default retrieves `25` records). You can request page = "all" but remember that this will make `r format(ceiling(results$results)/25, nsmall =0)` requests. This could take a while and might make sense to parallelize the request or split the calls so as to avoid hammering the server all at once.

Now we can examine the data itself.

```{r}
head(results$data)
```

We can also aggregate sensor data for any of the above mentioned sensors. We do this using the `ee_sensor_agg()` function. The function requires a sensor id and how the data should be binned. You can specify hours, minutes, seconds, days, weeks, month, and years. If for example you need the data binned every `15` days, simply add `days = 15` to the call. Once every `10` days and `2` hours would be `ee_sensor_agg(sensor_id = 1625, days = 10, hours = 2)` 

```{r}
stations <- ee_list_sensors()
# This gives you a list to choose from
sensor_df <- ee_sensor_agg(sensor_id = stations[1, c("record")], weeks = 2)
sensor_df
head(sensor_df$data)
```  

As with other functions, the results are paginated. So the full dataset can be retrieved by adding page = all to the query. If you think this might be unusally large, simply request the first page (default action) and look at the number of results. This divided by 25 (the number of observations per request) is how many API calls you will make. if this is less than `30`, it will run very quickly. If this is over a 100, your best course of action would be to parallelize the request or split them up. Since we only need `85` records in this case:

```{r}
sensor_df <- ee_sensor_agg(sensor_id = 1625, weeks = 2, page = "all")
sensor_df
```


### Searching the engine  

How to search the engine. Some notes on elastic search.

```{r, eval = FALSE}
ee_search()
ee_search_obs_get()
ee_search()
```

---


### Miscellaneous functions

__Footprints__

```{r, footprints_notrun, results = "asis", eval = FALSE, echo = TRUE}
footprints <- ee_footprints()
footprints[, -3] # To keep the table from spilling over
```

```{r, footprints, results = "asis", echo = FALSE}
footprints <- ee_footprints()
pandoc.table(footprints[, -3], justify = "left") # To keep the table from spilling over
```


__Data sources__

To obtain a list of data sources for the specimens contained in the museum.

```{r, results = "asis", eval = FALSE}
source_list <- ee_sources()
unique(source_list$name)
```

```{r, results = "asis", echo = FALSE}
source_list <- ee_sources()
pandoc.table(data.frame(name = unique(source_list$name)), justify = "left")
```



Please send any comments, questions, or ideas for new functionality or improvements to <[karthik.ram@berkeley.edu](karthik.ram@berkeley.edu)>. The code lives on GitHub [under the rOpenSci account](https://github.com/ropensci/ecoengine). Pull requests and [bug reports](https://github.com/ropensci/ecoengine/issues?state=open) are most welcome.

 Karthik Ram  
 January, 2014  
 _Berkeley, CA_
